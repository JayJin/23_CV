{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#Hough transform\n",
    "#Harris CD\n",
    "#Blob detection\n",
    "#pyramid\n",
    "\n",
    "#Data prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12b0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "def imgshow(inimg):\n",
    "    imgcv_rgb = cv2.cvtColor(inimg.copy(), cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(imgcv_rgb)\n",
    "    plt.show()\n",
    "\n",
    "cat = cv2.imread('cat.jpg')\n",
    "edge = cv2.imread('edge.jpg')\n",
    "fox = cv2.imread('fox.jpg')\n",
    "\n",
    "imgshow(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5ef4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = edge.copy()\n",
    "\n",
    "height, width = img.shape[:2]\n",
    "M = cv2.getRotationMatrix2D((width/2, height/2), 0, 0.8)\n",
    "img = cv2.warpAffine(img, M,(width, height))\n",
    "\n",
    "#Gray변환 후 엣지 검출\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray,50,150)\n",
    "\n",
    "#허프 라인 검출, 직선으로 판단할 최소한의 동일 개수 = 150px\n",
    "lines = cv2.HoughLines(edges,1,np.pi/180,150)\n",
    "print(lines.shape) #(N, 1, 2)\n",
    "\n",
    "for line in lines:\n",
    "    rho, theta = line[0] #Distance, angle return\n",
    "    a = np.cos(theta)\n",
    "    b = np.sin(theta)\n",
    "    x0 = a*rho\n",
    "    y0 = b*rho #x, y 절편 coord\n",
    "    x1 = int(x0 + 1000*(-b))\n",
    "    y1 = int(y0 + 1000*(a))\n",
    "    x2 = int(x0 - 1000*(-b))\n",
    "    y2 = int(y0 - 1000*(a))\n",
    "    #line 기준점 연산\n",
    "    cv2.line(img,(x1,y1),(x2,y2),(0,0,255),1)\n",
    "    \n",
    "imgshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e294f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = edge.copy()\n",
    "\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray,50,150)\n",
    "minLineLength = 100\n",
    "maxLineGap = 0\n",
    "\n",
    "lines = cv2.HoughLinesP(edges,1,np.pi/360,100,minLineLength,maxLineGap)\n",
    "print(lines.shape) #(N, 1, 4 - x1, y1, x2, y2: 시작과 끝 좌표)\n",
    "\n",
    "for line in lines:\n",
    "    for x1,y1,x2,y2 in line:\n",
    "        cv2.line(img,(x1,y1),(x2,y2),(0,0,255),2)\n",
    "        \n",
    "imgshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e891d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cat.copy()\n",
    "\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "blur = cv2.medianBlur(gray, 5)\n",
    "circles = cv2.HoughCircles(blur, cv2.HOUGH_GRADIENT, 1, 20,param1=50,param2=50,minRadius=100, maxRadius=0)\n",
    "\n",
    "if circles is not None:\n",
    "    circles = np.uint16(np.around(circles))\n",
    "    for i in circles[0,:]:\n",
    "        # 원 둘레에 초록색 원 그리기\n",
    "        cv2.circle(img,(i[0], i[1]), i[2], (0, 255, 0), 2)\n",
    "        # 원 중심점에 빨강색 원 그리기\n",
    "        cv2.circle(img, (i[0], i[1]), 2, (0,0,255), 5)\n",
    "        \n",
    "imgshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac5629",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = edge.copy()\n",
    "\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "dst = cv2.cornerHarris(gray,2,3,0.04)\n",
    "\n",
    "#Threshold for an optimal value\n",
    "img[dst>0.1*dst.max()]=[0,0,255]\n",
    "# 변화량 결과 최대값의 10% 이상의 좌표 빨갛게\n",
    "# same as np.where(dst>0.1*dst.max())\n",
    "\n",
    "imgshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf26a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = edge.copy()\n",
    "\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "dst = cv2.goodFeaturesToTrack(gray, 80, 0.01, 10)\n",
    "print(dst.shape)\n",
    "for corner in dst:\n",
    "    corner = np.uint16(np.around(corner))\n",
    "    cv2.circle(img, corner[0], 2, (0,0,255), 5)\n",
    "\n",
    "imgshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46e74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cat.copy()\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "detector = cv2.GFTTDetector_create() \n",
    "keypoint = detector.detect(gray, None)\n",
    "imgfe = cv2.drawKeypoints(img, keypoint, None)\n",
    "\n",
    "imgshow(imgfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c0a2bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img = cat.copy()\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "detector = cv2.FastFeatureDetector_create()\n",
    "keypoint = detector.detect(gray, None)\n",
    "imgfe = cv2.drawKeypoints(img, keypoint, None)\n",
    "\n",
    "imgshow(imgfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371191c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cat.copy()\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "detector = cv2.SimpleBlobDetector_create()\n",
    "#SimpleBlobDetector_Params로 별도 지정 가능!\n",
    "keypoint = detector.detect(gray, None)\n",
    "imgfe = cv2.drawKeypoints(img, keypoint, None)\n",
    "\n",
    "imgshow(imgfe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445715f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = fox.copy()\n",
    "\n",
    "down = cv2.pyrDown(img) # img x 1/4\n",
    "up = cv2.pyrUp(img) # img x 4\n",
    "\n",
    "imgarr = [img, down, up]\n",
    "\n",
    "for i in imgarr:\n",
    "    imgshow(i)\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = fox.copy()\n",
    "\n",
    "down = cv2.pyrDown(img) # img x 1/4\n",
    "up = cv2.pyrUp(down) # img x 4\n",
    "\n",
    "imgshow(up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = fox.copy()\n",
    "\n",
    "down = cv2.pyrDown(img)\n",
    "up = cv2.pyrUp(down)\n",
    "\n",
    "# 원본에서 확대 subtract\n",
    "laplacian = cv2.subtract(img, up)\n",
    "# 확대 이미지에 라플라시안 더해서 복원\n",
    "img = up + laplacian\n",
    "\n",
    "imgshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4046b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import data, io, filters\n",
    "\n",
    "sfox = io.imread('fox.jpg')\n",
    "io.imshow(sfox)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9179032",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.coins()\n",
    "# ... or any other NumPy array!\n",
    "edges = filters.sobel(image)\n",
    "io.imshow(edges)\n",
    "io.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c52178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "print(digits.images.shape)\n",
    "\n",
    "_, axes = plt.subplots(nrows=1, ncols=10, figsize=(10, 3))\n",
    "for ax, image, label in zip(axes, digits.images, digits.target):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(\"Im: %i\" % label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1)) #flatten\n",
    "print(data.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, digits.target, test_size=0.5, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c5999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import warp, AffineTransform, ProjectiveTransform\n",
    "from skimage.exposure import equalize_adapthist, equalize_hist, rescale_intensity, adjust_gamma, adjust_log, adjust_sigmoid\n",
    "from skimage.filters import gaussian\n",
    "from skimage.util import random_noise\n",
    "import random\n",
    "\n",
    "def randRange(a, b):\n",
    "    return random.uniform(a, b)\n",
    "\n",
    "def randomAffine(im):\n",
    "    tform = AffineTransform(scale=(randRange(0.75, 1.3), randRange(0.75, 1.3)),\n",
    "                            rotation=randRange(-0.25, 0.25),\n",
    "                            shear=randRange(-0.2, 0.2),\n",
    "                            translation=(randRange(-im.shape[0]//10, im.shape[0]//10), \n",
    "                                         randRange(-im.shape[1]//10, im.shape[1]//10)))\n",
    "    return warp(im, tform.inverse, mode='reflect')\n",
    "\n",
    "def randomPerspective(im):\n",
    "    region = 1/4\n",
    "    A = np.array([[0, 0], [0, im.shape[0]], [im.shape[1], im.shape[0]], [im.shape[1], 0]])\n",
    "    B = np.array([[int(randRange(0, im.shape[1] * region)), int(randRange(0, im.shape[0] * region))], \n",
    "                  [int(randRange(0, im.shape[1] * region)), int(randRange(im.shape[0] * (1-region), im.shape[0]))], \n",
    "                  [int(randRange(im.shape[1] * (1-region), im.shape[1])), int(randRange(im.shape[0] * (1-region), im.shape[0]))], \n",
    "                  [int(randRange(im.shape[1] * (1-region), im.shape[1])), int(randRange(0, im.shape[0] * region))], \n",
    "                 ])\n",
    "\n",
    "    pt = ProjectiveTransform()\n",
    "    pt.estimate(A, B)\n",
    "    return warp(im, pt, output_shape=im.shape[:2])\n",
    "\n",
    "def randomIntensity(im):\n",
    "    p_low, p_high = np.percentile(im, (1, 95))\n",
    "    return rescale_intensity(im, in_range=(p_low, p_high))\n",
    "\n",
    "def randomGamma(im):\n",
    "    return adjust_gamma(im, gamma=randRange(0.5, 1.5))\n",
    "\n",
    "def randomGaussian(im):\n",
    "    return gaussian(im, sigma=random.randrange(0, 5))\n",
    "    \n",
    "def randomFilter(im):\n",
    "    Filters = [equalize_adapthist, equalize_hist, adjust_log, adjust_sigmoid, randomGamma, randomGaussian, randomIntensity]\n",
    "    filt = random.choice(Filters)\n",
    "    return filt(im)\n",
    "\n",
    "def randomNoise(im):\n",
    "    var = randRange(0.001, 0.01)\n",
    "    return random_noise(im, var=var)\n",
    "    \n",
    "def augment(im, Steps=[randomAffine, randomPerspective, randomFilter, randomNoise]):\n",
    "    F = plt.figure(figsize=(15,9))\n",
    "    G = plt.GridSpec(2, 3, figure=F)\n",
    "    ax = plt.subplot(G[0])\n",
    "    ax.imshow(im)\n",
    "    ax.set_title('original')\n",
    "    for i, step in enumerate(Steps):\n",
    "        im = step(im)\n",
    "        ax = plt.subplot(G[i+1])\n",
    "        ax.imshow(im)\n",
    "        ax.set_title(step.__name__)\n",
    "    #return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1254f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "augment(sfox.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a43a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(\"Train, Test len\",len(trainset),len(testset))\n",
    "\n",
    "train_real, valset = torch.utils.data.random_split(trainset, [40000,10000])\n",
    "\n",
    "bat_size = 10\n",
    "trainloader = torch.utils.data.DataLoader(train_real, batch_size=bat_size, shuffle=True, num_workers=2)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=bat_size, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bat_size, shuffle=False, num_workers=2)\n",
    "print(\"Train, Val, Test size with batch:\",len(trainloader),len(valloader),len(testloader))\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2885c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code from DCGAN pytorch, utils.py\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "def get_celeba(params):\n",
    "    # Data proprecessing.\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(params['imsize']),\n",
    "        transforms.CenterCrop(params['imsize']),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5),\n",
    "            (0.5, 0.5, 0.5))])\n",
    "\n",
    "    # Create the dataset.\n",
    "    root = params['dataset']\n",
    "    print(root)\n",
    "    dataset = dset.ImageFolder(root=root, transform=transform)\n",
    "\n",
    "    # Create the dataloader.\n",
    "    dataloader = torch.utils.data.DataLoader(dataset,\n",
    "        batch_size=params['bsize'],\n",
    "        shuffle=True)\n",
    "\n",
    "    return dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38def",
   "language": "python",
   "name": "def"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
